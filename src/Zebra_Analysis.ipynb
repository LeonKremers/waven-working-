{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a009d4",
   "metadata": {},
   "source": [
    "# Init Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61bcb304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Waven.WaveletGenerator as wg\n",
    "import Waven.Analysis_Utils as au\n",
    "import Waven.LoadPinkNoise as lpn\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import torch\n",
    "\n",
    "results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665fccb2",
   "metadata": {},
   "source": [
    "# Create Zebra Noise Stimulus (only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ae7e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_stim = False\n",
    "\n",
    "if create_stim:\n",
    "    import zebranoise\n",
    "\n",
    "    #create a zebra noise video\n",
    "    zebranoise.zebra_noise(\"fullscreen_zebra.mp4\", xsize=800, ysize=600, tdur=60*15, fps=60, seed=7)\n",
    "\n",
    "# see options below for more examples\n",
    "#zebranoise.zebra_noise(\n",
    "# \"output2.mp4\", \n",
    "# xsize=640, \n",
    "# ysize=480, \n",
    "# tdur=60*2, \n",
    "# levels=10, \n",
    "# xyscale=.2, \n",
    "# tscale=50, \n",
    "# fps=30, \n",
    "# xscale=1.0, \n",
    "# yscale=1.0, \n",
    "# seed=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb27a8a",
   "metadata": {},
   "source": [
    "# Create Gabor Library (only once)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efaf3ae",
   "metadata": {},
   "source": [
    "#### Gabor Library Parameters:\n",
    "- **N_thetas** (int): Number of orientations equally spaced between 0 and 180 degrees.\n",
    "- **Sigmas** (list): Standard deviation of the Gabor filters expressed in pixels (radius of the Gaussian half peak width).\n",
    "- **Frequencies** (list): Spatial frequencies expressed in pixels per cycle.\n",
    "- **Phases** (list): 0 and π/2.\n",
    "- **NX** (int): Number of azimuth positions (pixels) (x shape of the downsampled stimuli).\n",
    "- **NY** (int): Number of elevation positions (pixels) (y shape of the downsampled stimuli).\n",
    "- **Save Path** (string): Where to save the Gabor library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a4c8241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of default parameters for the Gabor Library\n",
    "gabor_param={\n",
    "    \"N_thetas\":\"6\",\n",
    "    \"Sigmas\": \"[2, 3, 4, 5, 6, 8]\",\n",
    "    \"Frequencies\": \"[0.015, 0.04, 0.07, 0.1]\",\n",
    "    \"Phases\": \"[0, 90]\",\n",
    "    \"NX\": \"100\",\n",
    "    \"NY\": \"75\",\n",
    "    \"Save Path\":\"/datajoint-data/data/leonk/analysis/zebra/gabor_library.npy\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52fee134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gabor library already exists at /datajoint-data/data/leonk/analysis/zebra/gabor_library.npy\n",
      "Library shape: (100, 75, 6, 6, 2, 7500)\n",
      "File size: 7724.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Generate Gabor library if it doesn't exist\n",
    "lib_path = gabor_param[\"Save Path\"]\n",
    "\n",
    "if not os.path.exists(lib_path):\n",
    "    print(f\"Generating Gabor library at {lib_path}...\")\n",
    "    \n",
    "    # Extract parameters\n",
    "    n_thetas = int(gabor_param[\"N_thetas\"])\n",
    "    sigmas = np.array(eval(gabor_param[\"Sigmas\"]))\n",
    "    frequencies = np.array(eval(gabor_param[\"Frequencies\"]))\n",
    "    phases = np.array(eval(gabor_param[\"Phases\"]))\n",
    "    nx = int(gabor_param[\"NX\"])\n",
    "    ny = int(gabor_param[\"NY\"])\n",
    "    \n",
    "    # Create coordinate arrays\n",
    "    xs = np.arange(nx)\n",
    "    ys = np.arange(ny)\n",
    "    thetas = np.linspace(0, np.pi, n_thetas, endpoint=False)\n",
    "    offsets = np.deg2rad(phases)  # Convert phases to radians\n",
    "    \n",
    "    print(f\"Parameters:\")\n",
    "    print(f\"  Grid: {nx}x{ny}\")\n",
    "    print(f\"  Orientations: {n_thetas}\")\n",
    "    print(f\"  Sigmas: {sigmas}\")\n",
    "    print(f\"  Frequencies: {frequencies}\")\n",
    "    print(f\"  Phases: {phases}°\")\n",
    "    print(f\"\\nThis may take several minutes...\")\n",
    "    \n",
    "    # Generate library - this will take time!\n",
    "    # Using the first frequency for the library\n",
    "    library = wg.makeFilterLibrary(xs, ys, thetas, sigmas, offsets, frequencies[0], freq=True)\n",
    "    \n",
    "    # Save the library\n",
    "    np.save(lib_path, library)\n",
    "    print(f\"\\nGabor library saved! Shape: {library.shape}\")\n",
    "    print(f\"File size: {os.path.getsize(lib_path) / (1024**2):.1f} MB\")\n",
    "else:\n",
    "    print(f\"Gabor library already exists at {lib_path}\")\n",
    "    library = np.load(lib_path)\n",
    "    print(f\"Library shape: {library.shape}\")\n",
    "    print(f\"File size: {os.path.getsize(lib_path) / (1024**2):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42af893d",
   "metadata": {},
   "source": [
    "# Run Zebra Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907343c2",
   "metadata": {},
   "source": [
    "Define Scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e29b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "scans = [\n",
    "        'LE_ROS-2210_2025-11-28_scan9FXEU7TJ_sess9FXEU7TJ',\n",
    "        'LE_ROS-2210_2025-11-30_scan9FXG17XS_sess9FXG17XS',\n",
    "        'LE_ROS-2210_2025-12-02_scan9FXH819L_sess9FXH819L'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68637960",
   "metadata": {},
   "source": [
    "### Parameters Documentation\n",
    "\n",
    "#### Alignment Parameters:\n",
    "- **Dirs** (string): Where the raw data are located.\n",
    "- **Experiment Info**: (mouse name, date, experiment number)\n",
    "- **Number of Planes** (int): Number of acquisition planes.\n",
    "- **Block End** (int): Timeframe where the experiment starts.\n",
    "- **Number of Frames** (int): Number of frames (stim 30 Hz → 1800 frames/min).\n",
    "- **Number of Trials to Keep** (int): Number of trials to keep.\n",
    "\n",
    "#### Analysis Parameters:\n",
    "- **screen_x**: Stimulus screen x size in pixels.\n",
    "- **screen_y**: Stimulus screen y size in pixels.\n",
    "- **NX** (int): Number of azimuth positions (pixels) (x shape of the downsampled stimuli).\n",
    "- **NY** (int): Number of elevation positions (pixels) (y shape of the downsampled stimuli).\n",
    "- **Resolution** (float): Microscope resolution (µm per pixel).\n",
    "- **Sigmas** (list): Standard deviation of the Gabor filters expressed in pixels (radius of the Gaussian half peak width).\n",
    "- **Visual Coverage** (list): [azimuth left, azimuth right, elevation top, elevation bottom] in visual degrees.\n",
    "- **Analysis Coverage** (list): [azimuth left, azimuth right, elevation top, elevation bottom] in visual degrees.\n",
    "- **Movie Path**: Path to the stimulus (.mp4).\n",
    "- **Library Path**: Path to Gabor library (same as save path if ran).\n",
    "- **Spks Path** (optional): Path to the spks.npy file to skip the alignment procedure. If set, ignores alignment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d71b6b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of parameters\n",
    "\n",
    "param_defaults = {\n",
    "    \"Dirs\": \"/datajoint-data/data/leonk/\",\n",
    "    \"Number of Planes\": \"1\",\n",
    "    \"Block End\": \"0\",\n",
    "    \"screen_x\":\"800\",\n",
    "    \"screen_y\":\"600\",\n",
    "    \"NX0\": \"100\", # downsampled x positions from Gabor library\n",
    "    \"NY0\": \"75\", # downsampled y positions from Gabor library\n",
    "    \"NX\": \"100\", # target downsampled x positions for wavelet computation\n",
    "    \"NY\": \"75\", # target downsampled y positions for wavelet computation\n",
    "    \"Resolution\":\"1.2\",\n",
    "    \"Sigmas\": \"[2, 3, 4, 5, 6, 8]\",\n",
    "    \"Frequencies\": \"[0.015, 0.04, 0.07, 0.1]\",\n",
    "    \"Visual Coverage\":\"[-42, 42, 35, -15]\",\n",
    "    \"Analysis Coverage\": \"[-42, 42, 35, -15]\",\n",
    "    \"Number of Frames\": \"54000\",  # stimulus frames in each trial\n",
    "    \"Number of Trials to Keep\": \"1\",\n",
    "    \"Movie Path\": \"/datajoint-data/data/leonk/analysis/zebra/fullscreen_zebra.mp4\",\n",
    "    \"Library Path\": \"/datajoint-data/data/leonk/analysis/zebra/gabor_library.npy\",\n",
    "    \"Spks Path\": \"None\",\n",
    "    \"Device\": \"cuda:3\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aac61c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-GUI analysis workflow\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def run_analysis(param_defaults, gabor_param):\n",
    "    \"\"\"\n",
    "    Run analysis without GUI - suitable for Jupyter notebooks and headless environments\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract parameters\n",
    "    dirs = [param_defaults[\"Dirs\"]]\n",
    "    sigmas = np.array(eval(param_defaults[\"Sigmas\"]))\n",
    "    visual_coverage = eval(param_defaults[\"Visual Coverage\"])\n",
    "    analysis_coverage = eval(param_defaults[\"Analysis Coverage\"])\n",
    "    n_planes = int(param_defaults[\"Number of Planes\"])\n",
    "    block_end = int(param_defaults[\"Block End\"])\n",
    "    screen_x = int(param_defaults[\"screen_x\"])\n",
    "    screen_y = int(param_defaults[\"screen_y\"])\n",
    "    nx0 = int(param_defaults[\"NX0\"])\n",
    "    ny0 = int(param_defaults[\"NY0\"])\n",
    "    nx = int(param_defaults[\"NX\"])\n",
    "    ny = int(param_defaults[\"NY\"])\n",
    "    ns = len(sigmas)\n",
    "    resolution = float(param_defaults[\"Resolution\"])\n",
    "    spks_path = param_defaults[\"Spks Path\"]\n",
    "    nb_frames = int(param_defaults[\"Number of Frames\"])\n",
    "    n_trial2keep = int(param_defaults[\"Number of Trials to Keep\"])\n",
    "    movpath = param_defaults[\"Movie Path\"]\n",
    "    lib_path = param_defaults[\"Library Path\"]\n",
    "    n_theta = int(gabor_param[\"N_thetas\"])\n",
    "    device = param_defaults[\"Device\"]\n",
    "    \n",
    "    screen_ratio = abs(visual_coverage[0] - visual_coverage[1]) / nx\n",
    "    xM, xm, yM, ym = analysis_coverage\n",
    "    \n",
    "    print(f\"Visual coverage: {visual_coverage}, Sigmas: {sigmas}, NS: {ns}\")\n",
    "    print(f\"Directories: {dirs}\")\n",
    "\n",
    "    #set device for torch\n",
    "    torch.cuda.set_device(device)\n",
    "    \n",
    "    # Build paths\n",
    "    pathdata = dirs[0] \n",
    "    pathsuite2p = pathdata + '/suite2p'\n",
    "    \n",
    "    deg_per_pix = abs(xM - xm) / nx\n",
    "    sigmas_deg = np.trunc(2 * deg_per_pix * sigmas * 100) / 100\n",
    "    \n",
    "    print(f\"Data path: {pathdata}\")\n",
    "    print(f\"Suite2p path: {pathsuite2p}\")\n",
    "    \n",
    "    # Load spike data\n",
    "    if spks_path == 'None':\n",
    "        print('Aligning data...')\n",
    "        spks, spks_n, neuron_pos = lpn.loadSPKMesoscope(dirs[0], pathsuite2p, block_end, n_planes, nb_frames,\n",
    "                                                        threshold=1.25, last=True, method='frame2ttl')\n",
    "        neuron_pos = lpn.correctNeuronPos(neuron_pos, resolution)\n",
    "        neuron_pos[:, 1] = abs(neuron_pos[:, 1] - np.max(neuron_pos[:, 1]))\n",
    "    else:\n",
    "        print(f'Loading spks file from {spks_path}')\n",
    "        try:\n",
    "            spks = np.load(spks_path)\n",
    "            parent_dir = os.path.dirname(spks_path)\n",
    "            neuron_pos = np.load(os.path.join(parent_dir, 'pos.npy'))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file: {e}\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"Spike data shape: {spks.shape}\")\n",
    "    print(f\"Neuron positions shape: {neuron_pos.shape}\")\n",
    "    \n",
    "    # Compute response reliability and skewness\n",
    "    print(\"Computing response reliability and skewness...\")\n",
    "    \n",
    "    # Check if we have multiple trials for reliability calculation\n",
    "    n_neurons = spks.shape[0]\n",
    "    if n_trial2keep > 1:\n",
    "        # Multiple trials - compute cross-trial reliability\n",
    "        respcorr = au.repetability_trial3(spks, neuron_pos, plotting=False)\n",
    "    else:\n",
    "        # Single trial - skip reliability, set to default values\n",
    "        print(\"Single trial detected - skipping cross-trial reliability calculation\")\n",
    "        respcorr = np.ones(n_neurons)  # No filtering based on reliability for single trial\n",
    "    \n",
    "    skewness = au.compute_skewness_neurons(spks, plotting=False)\n",
    "    skewness = np.array(skewness)\n",
    "    \n",
    "    # Create filter mask\n",
    "    if n_trial2keep > 1:\n",
    "        filter_mask = np.logical_and(respcorr >= 0.2, skewness <= 20)\n",
    "    else:\n",
    "        # For single trial, only filter by skewness\n",
    "        filter_mask = skewness <= 20\n",
    "    \n",
    "    print(f\"Neurons passing filter: {np.sum(filter_mask)}/{n_neurons}\")\n",
    "    \n",
    "    # Plot neuron positions\n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 8))\n",
    "    ax1.scatter(neuron_pos[:, 0], neuron_pos[:, 1], c='k', alpha=0.5, s=30)\n",
    "    ax1.set_xlabel('X position (um)')\n",
    "    ax1.set_ylabel('Y position (um)')\n",
    "    ax1.set_title(\"Neuron Positions\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Load wavelets\n",
    "    parent_dir = os.path.dirname(movpath)\n",
    "    print(f\"Loading wavelets from {parent_dir}...\")\n",
    "    \n",
    "    # First try: load pre-computed downsampled wavelets\n",
    "    try:\n",
    "        wavelets_downsampled = np.load(os.path.join(parent_dir, 'dwt_downsampled_videodata.npy'))\n",
    "        w_r_downsampled = wavelets_downsampled[0]\n",
    "        w_i_downsampled = wavelets_downsampled[1]\n",
    "        w_c_downsampled = wavelets_downsampled[2]\n",
    "        del wavelets_downsampled\n",
    "        gc.collect()\n",
    "        print(\"Loaded downsampled wavelets\")\n",
    "    except Exception as e:\n",
    "        print(f\"Downsampled wavelets not found: {e}\")\n",
    "        \n",
    "        # Second try: load coarse wavelets\n",
    "        try:\n",
    "            print(\"Attempting to load coarse wavelets...\")\n",
    "            w_r_downsampled, w_i_downsampled, w_c_downsampled = lpn.coarseWavelet(parent_dir, False, nx0, ny0, nx, ny,\n",
    "                                                                                  n_theta, ns)\n",
    "            print(\"Loaded coarse wavelets\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading wavelets: {e}\")\n",
    "            \n",
    "            # Third try: Check if downsampled video exists and generate wavelets\n",
    "            downsampled_video_path = movpath[:-4] + '_downsampled.npy'\n",
    "            if os.path.exists(downsampled_video_path):\n",
    "                print(f\"Found downsampled video at {downsampled_video_path}\")\n",
    "                print(\"Generating wavelet decomposition from downsampled video...\")\n",
    "                try:\n",
    "                    videodata = np.load(downsampled_video_path)\n",
    "                    print(f\"Video data shape: {videodata.shape}\")\n",
    "                    \n",
    "                    wg.waveletDecomposition(videodata, 0, sigmas, parent_dir, library_path=lib_path)\n",
    "                    wg.waveletDecomposition(videodata, 1, sigmas, parent_dir, library_path=lib_path)\n",
    "                    \n",
    "                    w_r_downsampled, w_i_downsampled, w_c_downsampled = lpn.coarseWavelet(parent_dir, False, nx0, ny0, nx, ny,\n",
    "                                                                                          n_theta, ns)\n",
    "                    print(\"Completed wavelet decomposition from existing downsampled video\")\n",
    "                except Exception as e2:\n",
    "                    raise RuntimeError(f\"Error in wavelet decomposition from downsampled video: {e2}\")\n",
    "            else:\n",
    "                # Fourth try: Full pipeline - downsample video then decompose\n",
    "                print(\"Attempting full video processing pipeline...\")\n",
    "                try:\n",
    "                    if (visual_coverage != analysis_coverage):\n",
    "                        visual_coverage_arr = np.array(visual_coverage)\n",
    "                        analysis_coverage_arr = np.array(analysis_coverage)\n",
    "                        ratio_x = 1 - ((visual_coverage_arr[0] - visual_coverage_arr[1]) - (\n",
    "                                analysis_coverage_arr[0] - analysis_coverage_arr[1])) / (\n",
    "                                          visual_coverage_arr[0] - visual_coverage_arr[1])\n",
    "                        ratio_y = 1 - ((visual_coverage_arr[2] - visual_coverage_arr[3]) - (\n",
    "                                analysis_coverage_arr[2] - analysis_coverage_arr[3])) / (\n",
    "                                          visual_coverage_arr[2] - visual_coverage_arr[3])\n",
    "                    else:\n",
    "                        ratio_x = 1\n",
    "                        ratio_y = 1\n",
    "                    \n",
    "                    print(f\"Downsampling video: {movpath}\")\n",
    "                    wg.downsample_video_binary(movpath, visual_coverage, analysis_coverage, shape=(ny, nx), chunk_size=1000,\n",
    "                                            ratios=(ratio_x, ratio_y))\n",
    "                    videodata = np.load(movpath[:-4] + '_downsampled.npy')\n",
    "                    print(f\"Downsampled video shape: {videodata.shape}\")\n",
    "                    \n",
    "                    wg.waveletDecomposition(videodata, 0, sigmas, parent_dir, library_path=lib_path)\n",
    "                    wg.waveletDecomposition(videodata, 1, sigmas, parent_dir, library_path=lib_path)\n",
    "                    \n",
    "                    w_r_downsampled, w_i_downsampled, w_c_downsampled = lpn.coarseWavelet(parent_dir, False, nx0, ny0, nx, ny,\n",
    "                                                                                          n_theta, ns)\n",
    "                    print(\"Completed full wavelet decomposition\")\n",
    "                except Exception as e3:\n",
    "                    raise RuntimeError(f\"Error in full pipeline: {e3}\")\n",
    "    \n",
    "    # Compute receptive fields using Pearson correlation\n",
    "    print(\"Computing receptive fields...\")\n",
    "    print(f\"w_c_downsampled shape: {w_c_downsampled.shape}\")\n",
    "    print(f\"Expected: (27300, 27, 11, {n_theta}, {ns})\")\n",
    "    \n",
    "    # Use the actual number of frames we have\n",
    "    n_frames_to_use = min(w_c_downsampled.shape[0], spks.shape[1])\n",
    "    print(f\"Using {n_frames_to_use} frames for RF calculation\")\n",
    "    \n",
    "    rfs_gabor = au.PearsonCorrelationPinkNoise(w_c_downsampled[:n_frames_to_use].reshape(n_frames_to_use, -1), \n",
    "                                                np.mean(spks[:, :n_frames_to_use], axis=0),\n",
    "                                                neuron_pos, nx, ny, ns, analysis_coverage, \n",
    "                                                screen_ratio, sigmas_deg, plotting=True)\n",
    "    \n",
    "    # Plot retinotopy maps\n",
    "    fig2, ax2 = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    maxes1 = rfs_gabor[2]\n",
    "    plt.rcParams['axes.facecolor'] = 'none'\n",
    "    \n",
    "    m = ax2[0, 0].scatter(neuron_pos[:, 0], neuron_pos[:, 1], s=10, c=maxes1[0], cmap='jet', alpha=filter_mask)\n",
    "    fig2.colorbar(m, ax=ax2[0, 0])\n",
    "    ax2[0, 0].set_title('Azimuth Preference (deg)')\n",
    "    ax2[0, 0].set_xlabel('X (um)')\n",
    "    ax2[0, 0].set_ylabel('Y (um)')\n",
    "    \n",
    "    m = ax2[0, 1].scatter(neuron_pos[:, 0], neuron_pos[:, 1], s=10, c=maxes1[1], cmap='jet_r', alpha=filter_mask)\n",
    "    fig2.colorbar(m, ax=ax2[0, 1])\n",
    "    ax2[0, 1].set_title('Elevation Preference (deg)')\n",
    "    ax2[0, 1].set_xlabel('X (um)')\n",
    "    ax2[0, 1].set_ylabel('Y (um)')\n",
    "    \n",
    "    m = ax2[1, 0].scatter(neuron_pos[:, 0], neuron_pos[:, 1], s=10, c=maxes1[2], cmap='hsv', alpha=filter_mask)\n",
    "    fig2.colorbar(m, ax=ax2[1, 0])\n",
    "    ax2[1, 0].set_title('Orientation Preference (deg)')\n",
    "    ax2[1, 0].set_xlabel('X (um)')\n",
    "    ax2[1, 0].set_ylabel('Y (um)')\n",
    "    \n",
    "    m = ax2[1, 1].scatter(neuron_pos[:, 0], neuron_pos[:, 1], s=10, c=maxes1[3], cmap='coolwarm', alpha=filter_mask)\n",
    "    fig2.colorbar(m, ax=ax2[1, 1])\n",
    "    ax2[1, 1].set_title('Preferred Size (deg)')\n",
    "    ax2[1, 1].set_xlabel('X (um)')\n",
    "    ax2[1, 1].set_ylabel('Y (um)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save results\n",
    "    print(\"Saving results...\")\n",
    "    save_path = dirs + \"/zebra/\"\n",
    "    np.save(os.path.join(save_path, 'correlation_matrix.npy'), rfs_gabor[0])\n",
    "    np.save(os.path.join(save_path, 'maxes_indices.npy'), rfs_gabor[1])\n",
    "    np.save(os.path.join(save_path, 'maxes_corrected.npy'), rfs_gabor[2])\n",
    "    \n",
    "    print(\"Analysis complete!\")\n",
    "    return {\n",
    "        'spks': spks,\n",
    "        'neuron_pos': neuron_pos,\n",
    "        'rfs_gabor': rfs_gabor,\n",
    "        'filter_mask': filter_mask,\n",
    "        'respcorr': respcorr,\n",
    "        'skewness': skewness\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d7f883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis pipeline for scan: LE_ROS-2210_2025-11-28_scan9FXEU7TJ_sess9FXEU7TJ\n",
      "Visual coverage: [-42, 42, 35, -15], Sigmas: [2 3 4 5 6 8], NS: 6\n",
      "Directories: ['/datajoint-data/data/leonk/LE_ROS-2210_2025-11-28_scan9FXEU7TJ_sess9FXEU7TJ']\n",
      "Data path: /datajoint-data/data/leonk/LE_ROS-2210_2025-11-28_scan9FXEU7TJ_sess9FXEU7TJ\n",
      "Suite2p path: /datajoint-data/data/leonk/LE_ROS-2210_2025-11-28_scan9FXEU7TJ_sess9FXEU7TJ/suite2p\n",
      "Aligning data...\n",
      "last session\n",
      "single plane\n",
      "single plane\n",
      "shape spks :  (564, 27300)\n",
      "neuron_pos spks :  (564, 2)\n",
      "Found 1 complete trials (54001 stimulus frames, 27300 imaging frames)\n",
      "Processing trial 1\n",
      "  Trial 1: 26963 imaging frames, 26963 timepoints\n",
      "Detected incomplete final trial\n",
      "Aligning trial 1: (564, 27300), max index: 27008\n",
      "  26963 frames, 26963 timepoints, spks shape: (564, 26963)\n",
      "Alignment complete: 1 trials processed\n",
      "data aligned\n",
      "Spike data shape: (1, 54000, 564)\n",
      "Neuron positions shape: (564, 2)\n",
      "Computing response reliability and skewness...\n",
      "Single trial detected - skipping cross-trial reliability calculation\n",
      "Neurons passing filter: 561/1\n",
      "Loading wavelets from /datajoint-data/data/leonk/analysis/zebra...\n",
      "Downsampled wavelets not found: [Errno 2] No such file or directory: '/datajoint-data/data/leonk/analysis/zebra/dwt_downsampled_videodata.npy'\n",
      "Attempting to load coarse wavelets...\n",
      "loading wavelets...\n",
      "downsampling\n",
      "Error loading wavelets: [Errno 2] No such file or directory: '/datajoint-data/data/leonk/analysis/zebra/dwt_videodata_0.npy'\n",
      "Found downsampled video at /datajoint-data/data/leonk/analysis/zebra/fullscreen_zebra_downsampled.npy\n",
      "Generating wavelet decomposition from downsampled video...\n",
      "Video data shape: (54000, 75, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wavelet transform (phase=0): 100%|██████████| 54000/54000 [00:56<00:00, 961.50it/s] \n",
      "Wavelet transform (phase=0): 100%|██████████| 54000/54000 [00:53<00:00, 1006.61it/s]\n",
      "Wavelet transform (phase=0): 100%|██████████| 54000/54000 [00:50<00:00, 1069.87it/s]\n",
      "Wavelet transform (phase=0): 100%|██████████| 54000/54000 [00:51<00:00, 1045.70it/s]\n",
      "Wavelet transform (phase=0): 100%|██████████| 54000/54000 [00:51<00:00, 1044.75it/s]\n",
      "Wavelet transform (phase=0): 100%|██████████| 54000/54000 [00:51<00:00, 1041.59it/s]\n",
      "Wavelet transform (phase=1): 100%|██████████| 54000/54000 [00:52<00:00, 1023.78it/s]\n",
      "Wavelet transform (phase=1): 100%|██████████| 54000/54000 [00:52<00:00, 1020.54it/s]\n",
      "Wavelet transform (phase=1): 100%|██████████| 54000/54000 [00:51<00:00, 1055.00it/s]\n",
      "Wavelet transform (phase=1): 100%|██████████| 54000/54000 [00:50<00:00, 1064.47it/s]\n",
      "Wavelet transform (phase=1): 100%|██████████| 54000/54000 [00:52<00:00, 1037.86it/s]\n",
      "Wavelet transform (phase=1): 100%|██████████| 54000/54000 [00:51<00:00, 1056.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading wavelets...\n",
      "downsampling\n"
     ]
    }
   ],
   "source": [
    "# Run the analysis for each scan\n",
    "for scan in scans:\n",
    "    scan_dir = \"/datajoint-data/data/leonk/\" + scan \n",
    "    param_defaults[\"Dirs\"] = scan_dir\n",
    "    print(\"Starting analysis pipeline for scan:\", scan)\n",
    "    results = run_analysis(param_defaults, gabor_param)\n",
    "\n",
    "    if results is not None:\n",
    "        print(\"Results keys:\", results.keys())\n",
    "        save_path = scan_dir + \"/zebra/analysis_results.npy\"\n",
    "        np.save(save_path, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae50d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the last results\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbaf1a7",
   "metadata": {},
   "source": [
    "# Check and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b16d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load results\n",
    "if results is None:\n",
    "    results = np.load('/home/leonk/waven-working-/temp/analysis_results.npy', allow_pickle=True).item()\n",
    "#show result keys\n",
    "print(\"Results keys:\", results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4610cc44",
   "metadata": {},
   "source": [
    "Plot neuron positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20042e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xos = results[\"neuron_pos\"][:,0]  # x position\n",
    "yos = results[\"neuron_pos\"][:,1]  # y position\n",
    "\n",
    "print(f\"x position range: {xos.min():.1f} to {xos.max():.1f}\")\n",
    "print(f\"y position range: {yos.min():.1f} to {yos.max():.1f}\")\n",
    "print(f\"Number of neurons: {len(xos)}\")\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Calculate density for contours\n",
    "xy = np.vstack([xos, yos])\n",
    "kde = gaussian_kde(xy)\n",
    "\n",
    "# Create grid for contour plot\n",
    "x_grid = np.linspace(min(xos) - 2, max(xos) + 2, 100)\n",
    "y_grid = np.linspace(min(yos) - 2, max(yos) + 2, 100)\n",
    "X, Y = np.meshgrid(x_grid, y_grid)\n",
    "positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "Z = kde(positions).reshape(X.shape)\n",
    "\n",
    "# Plot density contours\n",
    "contours = ax.contour(X, Y, Z, levels=5, colors='gray', alpha=0.5, linewidths=0.8)\n",
    "ax.clabel(contours, inline=True, fontsize=8)\n",
    "\n",
    "# Overlay scatter plot\n",
    "ax.scatter(xos, yos, color='steelblue', s=50, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('x position', fontsize=11)\n",
    "ax.set_ylabel('y position', fontsize=11)\n",
    "ax.set_title('Neuron positions', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce3686",
   "metadata": {},
   "source": [
    "Plot RF positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ba67b5",
   "metadata": {},
   "source": [
    "Looking at the code, results[\"rfs_gabor\"] contains the output from PearsonCorrelationPinkNoise, which computes receptive fields. Based on how it's used in the plotting section, rfs_gabor is a tuple/list with 3 elements:\n",
    "\n",
    "rfs_gabor[0] - Correlation matrix\n",
    "\n",
    "Full correlation values between each neuron and each Gabor filter feature\n",
    "Shape: (n_neurons, n_gabor_features) where n_gabor_features = 27 × 11 × n_theta × ns\n",
    "These are the raw Pearson correlation coefficients\n",
    "rfs_gabor[1] - Maximum indices\n",
    "\n",
    "For each neuron, the indices of the Gabor features that gave the maximum correlation\n",
    "Shape: (n_neurons, 4) where the 4 dimensions correspond to:\n",
    "Azimuth position index (0-26)\n",
    "Elevation position index (0-10)\n",
    "Orientation index (0-7)\n",
    "Size/scale index (0-5)\n",
    "rfs_gabor[2] - Maxes corrected\n",
    "\n",
    "The actual preference values converted to meaningful units:\n",
    "maxes1[0]: Azimuth preference in degrees\n",
    "maxes1[1]: Elevation preference in degrees\n",
    "maxes1[2]: Orientation preference in degrees (0-180°)\n",
    "maxes1[3]: Preferred size in degrees (sigma of Gabor)\n",
    "Shape: (4, n_neurons)\n",
    "These are the values displayed in the retinotopy maps\n",
    "So in summary:\n",
    "\n",
    "[0] = all correlations (for detailed analysis)\n",
    "[1] = best-matching feature indices (for reconstruction)\n",
    "[2] = preferred stimulus properties in degrees (for visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d4da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xos = results[\"rfs_gabor\"][2][0]  # Azimuth preference in degrees\n",
    "yos = results[\"rfs_gabor\"][2][1]  # Elevation preference in degrees\n",
    "\n",
    "print(f\"Azimuth range: {xos.min():.1f} to {xos.max():.1f} degrees\")\n",
    "print(f\"Elevation range: {yos.min():.1f} to {yos.max():.1f} degrees\")\n",
    "print(f\"Number of neurons: {len(xos)}\")\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Calculate density for contours\n",
    "xy = np.vstack([xos, yos])\n",
    "kde = gaussian_kde(xy)\n",
    "\n",
    "# Create grid for contour plot\n",
    "x_grid = np.linspace(min(xos) - 2, max(xos) + 2, 100)\n",
    "y_grid = np.linspace(min(yos) - 2, max(yos) + 2, 100)\n",
    "X, Y = np.meshgrid(x_grid, y_grid)\n",
    "positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "Z = kde(positions).reshape(X.shape)\n",
    "\n",
    "# Plot density contours\n",
    "contours = ax.contour(X, Y, Z, levels=5, colors='gray', alpha=0.5, linewidths=0.8)\n",
    "ax.clabel(contours, inline=True, fontsize=8)\n",
    "\n",
    "# Overlay scatter plot\n",
    "ax.scatter(xos, yos, color='steelblue', s=50, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('Azimuth (°)', fontsize=11)\n",
    "ax.set_ylabel('Elevation (°)', fontsize=11)\n",
    "ax.set_title('RF centers scatter', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zebra-noise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
