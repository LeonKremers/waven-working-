{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d1cca3",
   "metadata": {},
   "source": [
    "# Gotta go fast!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a009d4",
   "metadata": {},
   "source": [
    "# Init Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61bcb304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Waven.WaveletGenerator as wg\n",
    "import Waven.Analysis_Utils as au\n",
    "import Waven.LoadPinkNoise as lpn\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import torch\n",
    "# Non-GUI analysis workflow\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b8f5205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preload_wavelets(param_defaults, gabor_param):\n",
    "    \"\"\"\n",
    "    Preload and cache wavelet decomposition - only needs to run once for the stimulus\n",
    "    Returns the wavelets that can be reused for all scans\n",
    "    \"\"\"\n",
    "    movpath = param_defaults[\"Movie Path\"]\n",
    "    lib_path = param_defaults[\"Library Path\"]\n",
    "    visual_coverage = eval(param_defaults[\"Visual Coverage\"])\n",
    "    analysis_coverage = eval(param_defaults[\"Analysis Coverage\"])\n",
    "    nx0 = int(param_defaults[\"NX0\"])\n",
    "    ny0 = int(param_defaults[\"NY0\"])\n",
    "    nx = int(param_defaults[\"NX\"])\n",
    "    ny = int(param_defaults[\"NY\"])\n",
    "    sigmas = np.array(eval(param_defaults[\"Sigmas\"]))\n",
    "    ns = len(sigmas)\n",
    "    n_theta = int(gabor_param[\"N_thetas\"])\n",
    "    \n",
    "    try:\n",
    "        device = param_defaults[\"Device\"]\n",
    "    except KeyError:\n",
    "        device = \"cuda:0\"\n",
    "    \n",
    "    torch.cuda.set_device(device)\n",
    "    parent_dir = os.path.dirname(movpath)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"PRELOADING WAVELETS (one-time operation)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # First try: load pre-computed downsampled wavelets\n",
    "    try:\n",
    "        wavelets_downsampled = np.load(os.path.join(parent_dir, 'dwt_downsampled_videodata.npy'))\n",
    "        w_r_downsampled = wavelets_downsampled[0]\n",
    "        w_i_downsampled = wavelets_downsampled[1]\n",
    "        w_c_downsampled = wavelets_downsampled[2]\n",
    "        del wavelets_downsampled\n",
    "        gc.collect()\n",
    "        print(\"✓ Loaded cached downsampled wavelets\")\n",
    "        return w_r_downsampled, w_i_downsampled, w_c_downsampled\n",
    "    except Exception as e:\n",
    "        print(f\"Cached wavelets not found: {e}\")\n",
    "    \n",
    "    # Second try: load coarse wavelets\n",
    "    try:\n",
    "        print(\"Attempting to load coarse wavelets...\")\n",
    "        w_r_downsampled, w_i_downsampled, w_c_downsampled = lpn.coarseWavelet(\n",
    "            parent_dir, False, nx0, ny0, nx, ny, n_theta, ns)\n",
    "        print(\"✓ Loaded coarse wavelets\")\n",
    "        return w_r_downsampled, w_i_downsampled, w_c_downsampled\n",
    "    except Exception as e:\n",
    "        print(f\"Coarse wavelets not found: {e}\")\n",
    "    \n",
    "    # Third try: Check if downsampled video exists and generate wavelets\n",
    "    downsampled_video_path = movpath[:-4] + '_downsampled.npy'\n",
    "    if os.path.exists(downsampled_video_path):\n",
    "        print(f\"✓ Found downsampled video at {downsampled_video_path}\")\n",
    "        print(\"Generating wavelet decomposition...\")\n",
    "        videodata = np.load(downsampled_video_path)\n",
    "        print(f\"  Video shape: {videodata.shape}\")\n",
    "        \n",
    "        wg.waveletDecomposition_batched(videodata, [0, 1], sigmas, parent_dir, \n",
    "                                       library_path=lib_path, device=device, batch_size=32)\n",
    "        \n",
    "        w_r_downsampled, w_i_downsampled, w_c_downsampled = lpn.coarseWavelet(\n",
    "            parent_dir, False, nx0, ny0, nx, ny, n_theta, ns)\n",
    "        print(\"✓ Completed wavelet decomposition\")\n",
    "        return w_r_downsampled, w_i_downsampled, w_c_downsampled\n",
    "    \n",
    "    # Fourth try: Full pipeline - downsample video then decompose\n",
    "    print(\"Running full pipeline: downsample video + wavelet decomposition...\")\n",
    "    \n",
    "    if visual_coverage != analysis_coverage:\n",
    "        visual_coverage_arr = np.array(visual_coverage)\n",
    "        analysis_coverage_arr = np.array(analysis_coverage)\n",
    "        ratio_x = 1 - ((visual_coverage_arr[0] - visual_coverage_arr[1]) - \n",
    "                      (analysis_coverage_arr[0] - analysis_coverage_arr[1])) / \\\n",
    "                      (visual_coverage_arr[0] - visual_coverage_arr[1])\n",
    "        ratio_y = 1 - ((visual_coverage_arr[2] - visual_coverage_arr[3]) - \n",
    "                      (analysis_coverage_arr[2] - analysis_coverage_arr[3])) / \\\n",
    "                      (visual_coverage_arr[2] - visual_coverage_arr[3])\n",
    "    else:\n",
    "        ratio_x = 1\n",
    "        ratio_y = 1\n",
    "    \n",
    "    print(f\"  Downsampling video: {movpath}\")\n",
    "    wg.downsample_video_binary(movpath, visual_coverage, analysis_coverage, \n",
    "                               shape=(ny, nx), chunk_size=1000, ratios=(ratio_x, ratio_y))\n",
    "    \n",
    "    videodata = np.load(movpath[:-4] + '_downsampled.npy')\n",
    "    print(f\"  Downsampled video shape: {videodata.shape}\")\n",
    "    \n",
    "    print(\"  Running wavelet decomposition...\")\n",
    "    wg.waveletDecomposition_batched(videodata, [0, 1], sigmas, parent_dir, \n",
    "                                   library_path=lib_path, device=device, batch_size=32)\n",
    "    \n",
    "    w_r_downsampled, w_i_downsampled, w_c_downsampled = lpn.coarseWavelet(\n",
    "        parent_dir, False, nx0, ny0, nx, ny, n_theta, ns)\n",
    "    \n",
    "    print(\"✓ Completed full wavelet pipeline\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return w_r_downsampled, w_i_downsampled, w_c_downsampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0527349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis_fast(scan_dir, w_c_preloaded, param_defaults, gabor_param, plot=True):\n",
    "    \"\"\"\n",
    "    Fast analysis using preloaded wavelets - only processes neural data\n",
    "    \n",
    "    Parameters:\n",
    "        scan_dir: Path to scan directory\n",
    "        w_c_preloaded: Preloaded wavelet tensor (complex wavelets)\n",
    "        param_defaults: Parameter dictionary\n",
    "        gabor_param: Gabor parameter dictionary\n",
    "        plot: Whether to display plots (set False for batch processing)\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Extract parameters\n",
    "    visual_coverage = eval(param_defaults[\"Visual Coverage\"])\n",
    "    analysis_coverage = eval(param_defaults[\"Analysis Coverage\"])\n",
    "    n_planes = int(param_defaults[\"Number of Planes\"])\n",
    "    block_end = int(param_defaults[\"Block End\"])\n",
    "    nx = int(param_defaults[\"NX\"])\n",
    "    ny = int(param_defaults[\"NY\"])\n",
    "    sigmas = np.array(eval(param_defaults[\"Sigmas\"]))\n",
    "    ns = len(sigmas)\n",
    "    resolution = float(param_defaults[\"Resolution\"])\n",
    "    spks_path = param_defaults[\"Spks Path\"]\n",
    "    nb_frames = int(param_defaults[\"Number of Frames\"])\n",
    "    n_trial2keep = int(param_defaults[\"Number of Trials to Keep\"])\n",
    "    n_theta = int(gabor_param[\"N_thetas\"])\n",
    "    \n",
    "    screen_ratio = abs(visual_coverage[0] - visual_coverage[1]) / nx\n",
    "    xM, xm, yM, ym = analysis_coverage\n",
    "    deg_per_pix = abs(xM - xm) / nx\n",
    "    sigmas_deg = np.trunc(2 * deg_per_pix * sigmas * 100) / 100\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ANALYZING: {os.path.basename(scan_dir)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Build paths\n",
    "    pathsuite2p = scan_dir + '/suite2p'\n",
    "    \n",
    "    # Load spike data\n",
    "    if spks_path == 'None':\n",
    "        print('⏳ Loading and aligning neural data...')\n",
    "        spks, spks_n, neuron_pos = lpn.loadSPKMesoscope(scan_dir, pathsuite2p, block_end, \n",
    "                                                        n_planes, nb_frames, threshold=1.25, \n",
    "                                                        last=True, method='frame2ttl')\n",
    "        neuron_pos = lpn.correctNeuronPos(neuron_pos, resolution)\n",
    "    else:\n",
    "        print(f'⏳ Loading spks from {spks_path}...')\n",
    "        spks = np.load(spks_path)\n",
    "        parent_dir = os.path.dirname(spks_path)\n",
    "        neuron_pos = np.load(os.path.join(parent_dir, 'pos.npy'))\n",
    "    \n",
    "    print(f\"  ✓ Neurons: {spks.shape[0]}, Frames: {spks.shape[1]}\")\n",
    "    \n",
    "    # Compute quality metrics\n",
    "    print('⏳ Computing neuron quality metrics...')\n",
    "    n_neurons = spks.shape[0]\n",
    "    \n",
    "    if n_trial2keep > 1:\n",
    "        respcorr = au.repetability_trial3(spks, neuron_pos, plotting=False)\n",
    "    else:\n",
    "        respcorr = np.ones(n_neurons)\n",
    "    \n",
    "    skewness = au.compute_skewness_neurons(spks, plotting=False)\n",
    "    skewness = np.array(skewness)\n",
    "    \n",
    "    if n_trial2keep > 1:\n",
    "        filter_mask = np.logical_and(respcorr >= 0.2, skewness <= 20)\n",
    "    else:\n",
    "        filter_mask = skewness <= 20\n",
    "    \n",
    "    print(f\"  ✓ Quality filter: {np.sum(filter_mask)}/{n_neurons} neurons passed\")\n",
    "    \n",
    "    # Compute receptive fields (this is the main computation)\n",
    "    print('⏳ Computing receptive fields...')\n",
    "    n_frames_to_use = min(w_c_preloaded.shape[0], spks.shape[1])\n",
    "    \n",
    "    rfs_gabor = au.PearsonCorrelationPinkNoise_batched(\n",
    "        stim=w_c_preloaded[:n_frames_to_use].reshape(n_frames_to_use, -1),\n",
    "        resp=spks[:, :n_frames_to_use],\n",
    "        neuron_pos=neuron_pos,\n",
    "        nx=nx, ny=ny, n_theta=n_theta, ns=ns,\n",
    "        visual_coverage=analysis_coverage,\n",
    "        screen_ratio=screen_ratio,\n",
    "        sigmas=sigmas_deg\n",
    "    )\n",
    "    \n",
    "    print(f\"  ✓ Receptive fields computed\")\n",
    "    \n",
    "    # Save results\n",
    "    print('⏳ Saving results...')\n",
    "    save_dir = scan_dir + \"/zebra/\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    np.save(os.path.join(save_dir, 'correlation_matrix.npy'), rfs_gabor[0])\n",
    "    np.save(os.path.join(save_dir, 'maxes_indices.npy'), rfs_gabor[1])\n",
    "    np.save(os.path.join(save_dir, 'maxes_corrected.npy'), rfs_gabor[2])\n",
    "    \n",
    "    results = {\n",
    "        'spks': spks,\n",
    "        'neuron_pos': neuron_pos,\n",
    "        'rfs_gabor': rfs_gabor,\n",
    "        'filter_mask': filter_mask,\n",
    "        'respcorr': respcorr,\n",
    "        'skewness': skewness\n",
    "    }\n",
    "    \n",
    "    np.save(os.path.join(save_dir, 'analysis_results.npy'), results)\n",
    "    print(f\"  ✓ Results saved to {save_dir}\")\n",
    "    \n",
    "    # Plot if requested\n",
    "    if plot:\n",
    "        print('⏳ Generating plots...')\n",
    "        fig2, ax2 = plt.subplots(2, 2, figsize=(14, 12))\n",
    "        maxes1 = rfs_gabor[2]\n",
    "        plt.rcParams['axes.facecolor'] = 'none'\n",
    "        \n",
    "        m = ax2[0, 0].scatter(neuron_pos[:, 0], neuron_pos[:, 1], s=10, c=maxes1[0], \n",
    "                             cmap='jet', alpha=filter_mask)\n",
    "        fig2.colorbar(m, ax=ax2[0, 0])\n",
    "        ax2[0, 0].set_title('Azimuth Preference (deg)')\n",
    "        ax2[0, 0].set_xlabel('X (um)')\n",
    "        ax2[0, 0].set_ylabel('Y (um)')\n",
    "        \n",
    "        m = ax2[0, 1].scatter(neuron_pos[:, 0], neuron_pos[:, 1], s=10, c=maxes1[1], \n",
    "                             cmap='jet_r', alpha=filter_mask)\n",
    "        fig2.colorbar(m, ax=ax2[0, 1])\n",
    "        ax2[0, 1].set_title('Elevation Preference (deg)')\n",
    "        ax2[0, 1].set_xlabel('X (um)')\n",
    "        ax2[0, 1].set_ylabel('Y (um)')\n",
    "        \n",
    "        m = ax2[1, 0].scatter(neuron_pos[:, 0], neuron_pos[:, 1], s=10, c=maxes1[2], \n",
    "                             cmap='hsv', alpha=filter_mask)\n",
    "        fig2.colorbar(m, ax=ax2[1, 0])\n",
    "        ax2[1, 0].set_title('Orientation Preference (deg)')\n",
    "        ax2[1, 0].set_xlabel('X (um)')\n",
    "        ax2[1, 0].set_ylabel('Y (um)')\n",
    "        \n",
    "        m = ax2[1, 1].scatter(neuron_pos[:, 0], neuron_pos[:, 1], s=10, c=maxes1[3], \n",
    "                             cmap='coolwarm', alpha=filter_mask)\n",
    "        fig2.colorbar(m, ax=ax2[1, 1])\n",
    "        ax2[1, 1].set_title('Preferred Size (deg)')\n",
    "        ax2[1, 1].set_xlabel('X (um)')\n",
    "        ax2[1, 1].set_ylabel('Y (um)')\n",
    "        \n",
    "        plt.suptitle(f\"{os.path.basename(scan_dir)}\", fontsize=14, y=0.995)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"✓ COMPLETE in {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a4c8241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of default parameters for the Gabor Library\n",
    "gabor_param={\n",
    "    \"N_thetas\":\"6\",\n",
    "    \"Sigmas\": \"[2, 3, 4, 5, 6, 8]\",\n",
    "    \"Frequencies\": \"[0.015, 0.04, 0.07, 0.1]\",\n",
    "    \"Phases\": \"[0, 90]\",\n",
    "    \"NX\": \"100\",\n",
    "    \"NY\": \"75\",\n",
    "    \"Save Path\":\"/datajoint-data/data/leonk/analysis/zebra/gabor_library.npy\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d71b6b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of parameters\n",
    "\n",
    "param_defaults = {\n",
    "    \"Dirs\": \"/datajoint-data/data/leonk/\",\n",
    "    \"Number of Planes\": \"1\",\n",
    "    \"Block End\": \"0\",\n",
    "    \"screen_x\":\"800\",\n",
    "    \"screen_y\":\"600\",\n",
    "    \"NX0\": \"100\", # downsampled x positions from Gabor library\n",
    "    \"NY0\": \"75\", # downsampled y positions from Gabor library\n",
    "    \"NX\": \"100\", # target downsampled x positions for wavelet computation\n",
    "    \"NY\": \"75\", # target downsampled y positions for wavelet computation\n",
    "    \"Resolution\":\"1.2\",\n",
    "    \"Sigmas\": \"[2, 3, 4, 5, 6, 8]\",\n",
    "    \"Frequencies\": \"[0.015, 0.04, 0.07, 0.1]\",\n",
    "    \"Visual Coverage\":\"[-42, 42, 35, -15]\",\n",
    "    \"Analysis Coverage\": \"[-42, 42, 35, -15]\",\n",
    "    \"Number of Frames\": \"54000\",  # stimulus frames in each trial\n",
    "    \"Number of Trials to Keep\": \"1\",\n",
    "    \"Movie Path\": \"/datajoint-data/data/leonk/analysis/zebra/fullscreen_zebra.mp4\",\n",
    "    \"Library Path\": \"/datajoint-data/data/leonk/analysis/zebra/gabor_library.npy\",\n",
    "    \"Spks Path\": \"None\",\n",
    "    \"Device\": \"cuda:1\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b87e8",
   "metadata": {},
   "source": [
    "Pre-load the Wavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9637f0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading wavelets for stimulus...\n",
      "============================================================\n",
      "PRELOADING WAVELETS (one-time operation)\n",
      "============================================================\n",
      "============================================================\n",
      "PRELOADING WAVELETS (one-time operation)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Run this once to preload wavelets\n",
    "print(\"Preloading wavelets for stimulus...\")\n",
    "w_r_global, w_i_global, w_c_global = preload_wavelets(param_defaults, gabor_param)\n",
    "print(f\"\\n✓ Wavelets cached in memory!\")\n",
    "print(f\"  Shape: {w_c_global.shape}\")\n",
    "print(f\"  Memory: ~{w_c_global.nbytes / 1e9:.2f} GB\")\n",
    "print(f\"  Ready for fast analysis on all scans!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907343c2",
   "metadata": {},
   "source": [
    "Define Scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e29b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "scans = [\n",
    "        'LE_ROS-2210_2025-11-28_scan9FXEU7TJ_sess9FXEU7TJ',\n",
    "        'LE_ROS-2210_2025-11-30_scan9FXG17XS_sess9FXG17XS',\n",
    "        'LE_ROS-2210_2025-12-02_scan9FXH819L_sess9FXH819L'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3968906d",
   "metadata": {},
   "source": [
    "# Fast Analysis Function (Uses Preloaded Wavelets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d7f883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAST BATCH PROCESSING - Run analysis on all scans using preloaded wavelets\n",
    "import time\n",
    "\n",
    "total_start = time.time()\n",
    "all_results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"BATCH PROCESSING {len(scans)} SCANS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, scan in enumerate(scans, 1):\n",
    "    scan_dir = \"/datajoint-data/data/leonk/\" + scan \n",
    "    \n",
    "    print(f\"\\n[{i}/{len(scans)}] Processing: {scan}\")\n",
    "    \n",
    "    # Run fast analysis with preloaded wavelets\n",
    "    results = run_analysis_fast(\n",
    "        scan_dir=scan_dir,\n",
    "        w_c_preloaded=w_c_global,  # Use the preloaded wavelets!\n",
    "        param_defaults=param_defaults,\n",
    "        gabor_param=gabor_param,\n",
    "        plot=True  # Set to False for even faster batch processing\n",
    "    )\n",
    "    \n",
    "    all_results[scan] = results\n",
    "\n",
    "total_elapsed = time.time() - total_start\n",
    "avg_time = total_elapsed / len(scans)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BATCH PROCESSING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total time: {total_elapsed:.1f}s ({total_elapsed/60:.1f} min)\")\n",
    "print(f\"Average per scan: {avg_time:.1f}s ({avg_time/60:.1f} min)\")\n",
    "print(f\"Scans processed: {len(all_results)}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zebra-noise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
